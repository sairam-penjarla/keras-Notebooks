{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning and Neural Network libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Add, MaxPooling2D, Dense, BatchNormalization, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns \n",
    "\n",
    "# SciPy and Sklearn for additional functionalities\n",
    "import scipy as sp\n",
    "from scipy import ndimage\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths for Dataset\n",
    "cat_file_path = \"/kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Cat\"\n",
    "dog_file_path = \"/kaggle/input/microsoft-catsvsdogs-dataset/PetImages/Dog\"\n",
    "# Dataset Class Visualization\n",
    "class_names = ['Cat', 'Dog'] \n",
    "\n",
    "n_dogs = len(os.listdir(cat_file_path))\n",
    "n_cats = len(os.listdir(dog_file_path))\n",
    "n_images = [n_cats, n_dogs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(main_dir, split_size=0.9):\n",
    "    files = []\n",
    "    for file in os.listdir(main_dir):\n",
    "        if  os.path.getsize(os.path.join(main_dir, file)): # check if the file's size isn't 0\n",
    "            files.append(file) # appends file name to a list\n",
    "\n",
    "    shuffled_files = random.sample(files, len(files)) # shuffles the data\n",
    "    split = int(split_size * len(shuffled_files)) # the training split casted into int for numeric rounding\n",
    "    train_files = shuffled_files[:split] # training split\n",
    "    split_valid_test = int(split + (len(shuffled_files) - split) / 2)\n",
    "\n",
    "    validation_files = shuffled_files[split:split_valid_test] # validation split\n",
    "    test_files = shuffled_files[split_valid_test:]\n",
    "\n",
    "    return train_files, validation_files, test_files\n",
    "\n",
    "# Split the data for cats and dogs\n",
    "train_cats, val_cats, test_cats = split_data(cat_file_path)\n",
    "train_dogs, val_dogs, test_dogs = split_data(dog_file_path)\n",
    "\n",
    "print(\"Cats - Training:\", len(train_cats), \"Validation:\", len(val_cats), \"Test:\", len(test_cats))\n",
    "print(\"Dogs - Training:\", len(train_dogs), \"Validation:\", len(val_dogs), \"Test:\", len(test_dogs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a dataframe from file names and labels\n",
    "def create_dataframe(file_names, label, main_dir):\n",
    "    return pd.DataFrame({\n",
    "        'filename': [os.path.join(main_dir, fname) for fname in file_names],\n",
    "        'class': label\n",
    "    })\n",
    "\n",
    "# Create dataframes\n",
    "train_df = pd.concat([create_dataframe(train_cats, 'cat', cat_file_path), create_dataframe(train_dogs, 'dog', dog_file_path)])\n",
    "val_df = pd.concat([create_dataframe(val_cats, 'cat', cat_file_path), create_dataframe(val_dogs, 'dog', dog_file_path)])\n",
    "test_df = pd.concat([create_dataframe(test_cats, 'cat', cat_file_path), create_dataframe(test_dogs, 'dog', dog_file_path)])\n",
    "\n",
    "# Shuffle the dataframes\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "val_df = val_df.sample(frac=1).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "validation_gen =  ImageDataGenerator(rescale=1./255.)\n",
    "test_gen =  ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "# Data Generators from Dataframes\n",
    "train_generator = train_gen.flow_from_dataframe(dataframe=train_df, x_col='filename', y_col='class', target_size=(150, 150), batch_size=64, class_mode='binary')\n",
    "validation_generator = validation_gen.flow_from_dataframe(dataframe=val_df, x_col='filename', y_col='class', target_size=(150, 150), batch_size=64, class_mode='binary')\n",
    "test_generator = test_gen.flow_from_dataframe(dataframe=test_df, x_col='filename', y_col='class', target_size=(150, 150), batch_size=64, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the CNN model\n",
    "inputs = tf.keras.layers.Input(shape=(150,150,3))\n",
    "# Convolutional layer with 32 filters of size 3x3, using ReLU activation function\n",
    "x = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(inputs)\n",
    "# Another Convolutional layer with increased filters for capturing more complex features\n",
    "x = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(x)\n",
    "# MaxPooling to reduce dimensionality and prevent overfitting\n",
    "x = tf.keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "# Additional Convolutional layers with increasing filter size for deeper feature extraction\n",
    "x = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(x)\n",
    "x = tf.keras.layers.Conv2D(128, (3,3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(2,2)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(128, (3,3), activation='relu')(x)\n",
    "x = tf.keras.layers.Conv2D(256, (3,3), activation='relu')(x)\n",
    "\n",
    "# Global Average Pooling layer for reducing overfitting and improving model efficiency\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Dense layers for classification\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(2, activation='softmax')(x) \n",
    "\n",
    "model = Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Compilation\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.RMSprop(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model Training\n",
    "r = model.fit(\n",
    "        train_generator,\n",
    "        epochs = 20, # Training longer could yield better results\n",
    "        validation_data=validation_generator)\n",
    "# Model Evaluation\n",
    "model.evaluate(test_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
